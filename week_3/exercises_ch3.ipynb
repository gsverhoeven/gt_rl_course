{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Chapter 3 Markov Decision Processes\n",
    "\n",
    "## p51 Ex 3.1 Devise three tasks in MDP framework\n",
    "\n",
    "A: **Task 1**: Painting the house properly in the shortest amount of time, because the scaffolding is rented.\n",
    "\n",
    "Actions: whether to stay inside, remove old paint or paint.\n",
    "Reward: -1 for each action.\n",
    "States of the world: a combination of the House state (with old paint, old paint removed, and with new paint on) and a weather forecast for the day after.\n",
    "\n",
    "**Task 2**: Deciding whether to work at the office or stay at home.\n",
    "\n",
    "Actions: work at home or go to work.\n",
    "\n",
    "States of the world: At home: with kids, without kids, messy house, clean house. \n",
    "Expectation for the office (just called with the office):  Almost empty, medium occupancy, high occupancy.\n",
    "\n",
    "COVID situation in the area: no COVID, low COVID, high COVID.\n",
    "\n",
    "Rewards at home are not a function of COVID. Rewards at the office ARE. for no COVID, reward goes up with occupancy.\n",
    "for low COVID, reward goes down, a lot for high occupancy. for high COVID, strong negative rewards for medium and high occupancy.\n",
    "\n",
    "**Task 3**: Landing a spacecraft on Mars.\n",
    "\n",
    "Action: whether to activate parachute, the jet thrusters.\n",
    "State of the world: sensor input (radar, visual)\n",
    "Succesful reward for succesfull landing. Large negative reward for crashing. Maybe also positive rewards for staying level? Risky.\n",
    "\n",
    "## Ex 3.2 Exceptions to MDP\n",
    "\n",
    "It appears that MDP cannot be UNCERTAIN about its state. I.e. it either is in a particular state, or it is not.\n",
    "Also, the role of other agents making decisions. We can of course envision the other agents as part of the environment.\n",
    "\n",
    "## Ex 3.3 The problem of driving\n",
    "\n",
    "What the right level or place to draw the line between agent and environment?\n",
    "I guess where we have control? If we have a robot sitting in the drivers seat, we want to control the actuators in the robot.\n",
    "If there is no driver and the car is supposed to drive by itself, the control is likely in some actuator rotating the steering column (https://en.wikipedia.org/wiki/Steering#/media/File:Steer_system.jpg)\n",
    "So we want to control that actuators voltages. \n",
    "\n",
    "## Ex. 3.6 Pole balancing (Cartpole!)\n",
    "\n",
    "## Ex 3.7 robot in maze\n",
    "\n",
    "## Ex 3.8 expected return G\n",
    "\n",
    "## Ex 3.9 expected return G\n",
    "\n",
    "## Ex 3.12 v_pi\n",
    "\n",
    "## Ex 3.14 verify bellman equation for gridworld example\n",
    "\n",
    "The bellman equation is a consistency relationship.\n",
    "It should always hold.\n",
    "\n",
    "It says that the value of a state under a given policy of choosing actions, is equal to\n",
    "the weighted average, using as weights the probability of choosing each action, of the weighted average of the reward (instant reward r and expected from the new state, with discounting factor gamma) \n",
    "of each state - reward combination we can get, using the probability of ending up in eacht state-reward as weights.\n",
    "\n",
    "Policy here is random action choice.\n",
    "Such p(a | s) = 0.25 for each action.\n",
    "and p(s' , r | s, a ) = 1 because for each action, we have a single new state, with a fixed reward (of zero).\n",
    "\n",
    "So... 0.7 = 0.9 (2.3 + 0.4 + -0.4 + 0.7) /4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9 * (2.3 + 0.4 + -0.4 + 0.7)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3.17 bellman equation for action values\n",
    "\n",
    "## Ex 3.20 optimal state-value for golf example\n",
    "\n",
    "## Ex 3.21 optimal action-value for putting in golf example\n",
    "\n",
    "## Ex 3.22 optimal policies for MDP\n",
    "\n",
    "## Ex 3.24 compute optimal value for best state in the Gridworld\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
