{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Multi-armed Bandits in the OpenAI Gym Environment\n",
    "\n",
    "## *by Gertjan Verhoeven (2020)*\n",
    "\n",
    "In this exercise, we are going to implement two algorithms to find the best slot machine (One-armed bandit) to play on.\n",
    "We will make use of a Python package, `gym_bandits` , that contains a series of n-armed bandit environments for the OpenAI Gym.\n",
    "We installed this package at the beginning of the course.\n",
    "\n",
    "Using a Gym Environment means that the multi armed bandit is already implemented in code, and we can focus on the algorithms to optimize our long run reward.\n",
    "\n",
    "First we will load the required libraries.\n",
    "We encounter **matplotlib** as a new package. You can use this package to make plots, similar to the `plot()`  function in R.\n",
    "Datacamp has a separate course for it, \"Introduction to data visualization with matplotlib\" .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_bandits\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gym-bandits` package contains an environment `BanditTenArmedGaussian-v0` that exactly matches what is described on page 30 of S&B.\n",
    "Lets make an instance of this environment.\n",
    "We start with setting a seed to make sure we can reproduce the environment.\n",
    "When the env is created, it generates 10 slot machines with average rewards drawn from a normal distribution.\n",
    "\n",
    "(Check `bandit.py` in the [GitHub repo for `gym-bandits`](https://github.com/JKCooper2/gym-bandits) if you are curious how the environment is coded up.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "env = gym.make(\"BanditTenArmedGaussian-v0\") \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what the output of a single action (step) is, we choose slot machine nr 1.\n",
    "Run the cell a few times to see what the variation in rewards is we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.9836979434943602, True, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `env.step()` call returns four objects.\n",
    "* The first element holds information about the state. Since the slot machine does not change, this value is always 0.\n",
    "* The second element holds the reward. \n",
    "* The third element informs us that the game has ended or not. `True` means the game has ended.\n",
    "* The fourth element may contain optional information about e.g. the state that the agent is not allowed to use for decision making.\n",
    "\n",
    "Now let us choose a different action (bandit arm), action 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -2.7210563412357827, True, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which action is better? Lets find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Playing all slot machines an equal number of times\n",
    "\n",
    "a) Run each slot machine a 1000 times by calling `env.step()` and store the results in a numpy array of dimensions `[10][1000]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use **matplotlib** to plot a violin plot of the distributions of the 10 slot machines, similar to Figure 2.1 in S&B. Which slot machine has the highest average reward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: A simple bandit algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We needed 10x1000 = 10.000 actions to find the best slot machine. Can we do it faster?\n",
    "\n",
    "Let's code up an algorithm that chooses the action with the highest Q-value with probability `1- eps` and chooses a random action with probability `eps`.\n",
    "\n",
    "a) If we chooses actions this way, for the case of 10 actions and epsilon = 0.1, what is the probability that the action with the highest Q-value is selected?\n",
    "\n",
    "The Q-value table holds estimates of the average reward for each action. We can either use the incremental method as described in section 2.4, e.g. a step size parameter that decreases as 1/n, or we can use a constant step size parameter as described in section 2.5. \n",
    "\n",
    "b)Given what you know about the Bandit Gym environment, which one is better? Why?\n",
    "\n",
    "c) Create an array of size 10, `q_table` to hold for each action its current estimated Q-value.\n",
    "Write a function `choose_action()` that takes as arguments \n",
    "\n",
    "* a `q_table` that corresponds to the Q-values of each action, \n",
    "* and the parameter `eps`.\n",
    "\n",
    "The function should first generate a uniform random number between 0 and 1. \n",
    "If this number is greater or equal than `eps`, choose the action with the highest Q-value. \n",
    "If not, choose an action at random.\n",
    "The function returns the number of the chosen action. \n",
    "The Q-table can be used to infer the number of actions to choose from.\n",
    "\n",
    "Test your function by running it a few times and checking its output.\n",
    "\n",
    "Hint: Useful Numpy functions include `np.random.rand()`, `np.random.choice()`, `np.max()` and `np.where()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "all_actions = range(action_size)\n",
    "q_table = np.arange(action_size)\n",
    "\n",
    "def choose_action():\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use this function to implement the Epsilon-greedy algorithm described in pseudo code on page 32 of Sutton & Barto.\n",
    "Initialize all Q-values at 0.\n",
    "Choose `step_size = 0.1`. The action that the `choose_action()` function returns can be used directly to call the gym function `env.step()` to collect a reward for that action.\n",
    "\n",
    "Hint: you can have a look at `ten_armed_testbed.py` for inspiration how to code things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE = 0.1 # or STEP_SIZE = ONE_OVER_N\n",
    "TIME_STEPS = 1000\n",
    "EPS = 0.1\n",
    "action_size = env.action_space.n\n",
    "actions = range(action_size)\n",
    "\n",
    "# init q_table\n",
    "q_table = np.zeros(action_size)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) use matplotlib to plot the estimated Q-values over the violin plot you created earlier of all the slot machines. Did the algorithm find the correct average rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Now change the action selection function such that it chooses the action with the highest Upper Confidence Bound. \n",
    "It should take as arguments `q_table` and `c_val`.\n",
    "\n",
    "Hint: create an `n_table` that contains for each action the number of times it was chosen. \n",
    "When implementing the formula on p 35, add a small numer like `1e-6` to Nt(a) to avoid dividing by zero. This simplifies your code.\n",
    "\n",
    "Test your function by feeding it known inputs and checking its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "n_table = np.zeros(action_size)\n",
    "q_table = np.arange(action_size)\n",
    "\n",
    "actions = range(action_size)\n",
    "\n",
    "def choose_action_ucb():\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Epsilon-greedy versus UCB: which is better?\n",
    "\n",
    "Let us create a simulation study where we can precisely measure the performance of our algorithms.\n",
    "The objective is to compare epsilon greedy with UCB, and make a plot similar to S&B p36.\n",
    "\n",
    "First, think about what you expect based on how the algorithms work. Which will reach the highest reward in the long run?\n",
    "\n",
    "a) Run the epsilon greedy algorithm 2000 times, each time for a 1000 steps. Store all rewards in a numpy array of size `[2000][1000]`.\n",
    "\n",
    "Do the same for the UCB algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = 2\n",
    "RUNS = 2000\n",
    "TIME_STEPS = 1000\n",
    "STEP_SIZE = 0.1 # or STEP_SIZE = ONE_OVER_N\n",
    "\n",
    "action_size = env.action_space.n\n",
    "actions = range(action_size)\n",
    "\n",
    "# define methods\n",
    "EPS_GREEDY = 0\n",
    "UCB = 1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For each algorithm, for each timestep, average the rewards over all runs. This shows how fast, on average, the algorithm finds the best slot machine.\n",
    "Plot the average rewards as a function of time steps for both algorithms in a single plot.\n",
    "\n",
    "Which one performs better? What performance criterium did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
